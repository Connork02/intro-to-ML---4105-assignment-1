import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Load the data from the CSV file using pandas
data = pd.read_csv('/content/D3.csv')

# Separate features (X1, X2, X3) and target (Y)
X = data[['X1', 'X2', 'X3']].values
Y = data['Y'].values

# Function to perform gradient descent
def gradient_descent(X, Y, learning_rate, num_iterations):
    m, n = X.shape  # Number of samples, number of features
    theta = np.zeros(n)  # Initialize parameters to zeros
    loss_history = []
    
    # Perform gradient descent
    for _ in range(num_iterations):
        # Compute predictions
        predictions = np.dot(X, theta)
        
        # Compute errors
        errors = predictions - Y
        
        # Compute loss
        loss = np.mean(errors ** 2) / 2
        loss_history.append(loss)
        
        # Compute gradient
        gradient = np.dot(X.T, errors) / m
        
        # Update parameters
        theta -= learning_rate * gradient
        
    return theta, loss_history

# Perform linear regression with gradient descent for each explanatory variable
learning_rates = [0.1, 0.05, 0.01]  # Different learning rates to explore
num_iterations = 1000  # Number of iterations

min_loss = float('inf')
best_explanatory_variable = None
best_loss_history = None

for i in range(X.shape[1]):  # Iterate over each explanatory variable
    for learning_rate in learning_rates:
        # Extract the current explanatory variable
        X_current = X[:, i:i+1]  # Extract ith column
        
        # Add a column of ones to X_current for the bias term
        X_current_bias = np.column_stack((np.ones(len(X_current)), X_current))
        
        # Perform gradient descent
        theta, loss_history = gradient_descent(X_current_bias, Y, learning_rate, num_iterations)
        
        # Plot the final regression model
        plt.scatter(X_current, Y, color='blue', label='Data')
        plt.plot(X_current, np.dot(X_current_bias, theta), color='red', label='Regression Model')
        plt.xlabel(f'Explanatory Variable: X{i+1}')
        plt.ylabel('Y')
        plt.title(f'Final Regression Model (Explanatory Variable: X{i+1}, Learning Rate: {learning_rate})')
        plt.legend()
        plt.show()
        
        # Plot loss over iterations
        plt.plot(loss_history)
        plt.xlabel('Iterations')
        plt.ylabel('Loss')
        plt.title(f'Loss over Iterations (Explanatory Variable: X{i+1}, Learning Rate: {learning_rate})')
        plt.show()
        
        # Report the linear model
        print(f"Explanatory Variable: X{i+1}, Learning Rate: {learning_rate}, Linear Model: Y = {theta[0]:.2f} + {theta[1]:.2f} * X{i+1}")
        
        # Track the minimum loss and the corresponding explanatory variable
        if loss_history[-1] < min_loss:
            min_loss = loss_history[-1]
            best_explanatory_variable = f'X{i+1}'
            best_loss_history = loss_history

# Report the explanatory variable with the lowest loss
print(f"The explanatory variable with the lowest loss is: {best_explanatory_variable}, with a final loss of: {min_loss}")
