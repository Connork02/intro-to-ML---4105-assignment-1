import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Load the data from the CSV file using pandas
data = pd.read_csv('/content/D3.csv')

# Separate features (X1, X2, X3) and target (Y)
X = data[['X1', 'X2', 'X3']].values
Y = data['Y'].values

# Function to add bias term to features
def add_bias(X):
    return np.column_stack((np.ones(len(X)), X))

# Function to perform gradient descent
def gradient_descent(X, Y, learning_rate, num_iterations):
    m, n = X.shape  # Number of samples, number of features
    theta = np.zeros(n)  # Initialize parameters to zeros
    loss_history = []
    
    # Perform gradient descent
    for _ in range(num_iterations):
        # Compute predictions
        predictions = np.dot(X, theta)
        
        # Compute errors
        errors = predictions - Y
        
        # Compute loss
        loss = np.mean(errors ** 2) / 2
        loss_history.append(loss)
        
        # Compute gradient
        gradient = np.dot(X.T, errors) / m
        
        # Update parameters
        theta -= learning_rate * gradient
        
    return theta, loss_history

# Perform linear regression with gradient descent for each learning rate
learning_rates = [0.1, 0.05, 0.01]  # Different learning rates to explore
num_iterations = 1000  # Number of iterations

best_loss = float('inf')
best_theta = None
best_learning_rate = None
best_loss_history = None

for learning_rate in learning_rates:
    # Add bias term to features
    X_with_bias = add_bias(X)
    
    # Perform gradient descent
    theta, loss_history = gradient_descent(X_with_bias, Y, learning_rate, num_iterations)
    
    # Check if this model has the lowest loss
    if loss_history[-1] < best_loss:
        best_loss = loss_history[-1]
        best_theta = theta
        best_learning_rate = learning_rate
        best_loss_history = loss_history

# Plot loss over iterations for the best learning rate
plt.plot(best_loss_history)
plt.xlabel('Iterations')
plt.ylabel('Loss')
plt.title(f'Loss over Iterations (Learning Rate: {best_learning_rate})')
plt.show()

# Report the final linear model
print(f"The final linear model with the lowest loss (Learning Rate: {best_learning_rate}) is:")
print(f"Y = {best_theta[0]:.2f} + {best_theta[1]:.2f} * X1 + {best_theta[2]:.2f} * X2 + {best_theta[3]:.2f} * X3")

# New values for (X1, X2, X3)
new_values = np.array([[1, 1, 1],
                       [2, 0, 4],
                       [3, 2, 1]])

# Add bias term to the new values
new_values_with_bias = add_bias(new_values)

# Predict the corresponding y values
predicted_y = np.dot(new_values_with_bias, best_theta)

# Print the predicted y values
print("\nPredicted y values for new (X1, X2, X3) values:")
for i in range(len(predicted_y)):
    print(f"For (X1, X2, X3) = ({new_values[i][0]}, {new_values[i][1]}, {new_values[i][2]}), Predicted Y = {predicted_y[i]:.2f}")
